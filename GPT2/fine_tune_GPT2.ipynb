{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "# For this example, we'll use the \"BookCorpus\" dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the OpenWebText dataset\n",
    "# Step 1: Load the dataset (only the 'train' split exists)\n",
    "raw_dataset = load_dataset(\"openwebtext\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the dataset into train and test (e.g., 90% train, 10% test)\n",
    "train_test_split = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "raw_train_dataset = train_test_split[\"train\"]\n",
    "raw_test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the tokenizer\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, TrainingArguments\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the tokenization function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Tokenize each split\n",
    "tokenized_train_dataset = raw_train_dataset.map(preprocess_function, batched=True, num_proc=4)\n",
    "tokenized_test_dataset = raw_test_dataset.map(preprocess_function, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Load Pre-Trained GPT2 Model\n",
    "# Load GPT-2 with a language modeling head\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust embedding size if tokenizer size changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",       # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate the model at the end of each epoch\n",
    "    learning_rate=5e-5,             # Learning rate\n",
    "    num_train_epochs=3,             # Number of training epochs\n",
    "    per_device_train_batch_size=2,  # Batch size per GPU/TPU\n",
    "    per_device_eval_batch_size=2,   # Batch size for evaluation\n",
    "    gradient_accumulation_steps=4,  # Simulate larger batch sizes\n",
    "    logging_dir=\"./logs\",           # Directory for logs\n",
    "    logging_steps=10,               # Log every 10 steps\n",
    "    save_steps = 500,               # Save checkpoint every 500 steps\n",
    "    save_total_limit=2,             # Only keep the last 2 checkpoints\n",
    "    load_best_model_at_end=True,    # Load the best model (based on evaluation metric)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Set up the trainer\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits,axis=-1)\n",
    "    # Calculate perplexity\n",
    "    perplexity = math.exp(np.mean([logit - label for logit, label in zip(logits.flatten(), labels.flatten())]))\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.1:Trainer Initialization\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    tokenizer=tokenizer,    # Optional: tokenizer for dynamic padding\n",
    "    compute_metrics=None,   # Replace with compute_metrics if desired\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Start the training process\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save the Fine-tuned Model\n",
    "model.save_pretrained(\"./fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Test the Fine-tuned Model\n",
    "\n",
    "# Load the fine-tuned model for text generation\n",
    "generator = pipeline(\"text-generation\", model=\"./fine_tuned_gpt2\", tokenizer=tokenizer)\n",
    "\n",
    "# Generate text\n",
    "text = generator(\"Once upon a time\", max_length=50, num_return_sequences=1)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
